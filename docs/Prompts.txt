  ❯ Ok, let's consider an all-encompassing test     
  suite that would rerun all those tests and      
  clear up the tests that failed due to a         
  pre-existing condition. This would include      
  tests that have been created before UR-001 was  
  done. Such a master test script would include   
  the ability to select among unit tests,         
  vitests and e2e tests (notably Playwright       
  tests) as well as an option (where possible)    
  to run with or without GCP setup as well as an  
  option to run tests manually or                 
  automaticdally (esp., e2e tests). Please        
  consider what the hi-level methology docs say   
  about this, viz.,                               
  @docs/Dylan-Davis-50plus-method.md and @docs/M  
  atts-integration-with-Dylan-plan-samkirk-v3.md .

❯ I believe that a separate task for this plan must            
  include whether to fix or delete pre v2-upgrade tests       
  that fail. The plan should at least identify them. In        
  some cases, fixing the plan may duplicate existing tests     
   that are working. Such investigation can be done later      
  after the master test script for know good tests is in       
  place.                                             

❯ We need to think about recording the test results as proof  
  positive of testing. Such test report should include a very  
  brief description of the test and the location of the test   
  code for auditing purposes. Although there is a              
  test-results folder, I think a better place would be in the  
  archive folder as separate test file (or folder of files)    
  that is date/time stamped as YYYY-MM-DD_HH:MM:SS using the   
  PST zone. It would be nice if those test results were        
  linked (or even cross-linked) with UR-001, but I don't know  
  how. I like the idea of keeping detailed test results out    
  of the UR folder because documenting ocassional test         
  results can be so awkward: see docs/TEST-RESULTS.md, for     
  example, and the dreadfully long logs in .playwright-mcp,    
  and also  the details of testing as reference (faux) test    
  fixturs in web/test-fixtures. In short, I find it difficult  
  to know what to keep and how to organize it, but Matt's      
  requirement of a traceability for auditing sure sounds like  
  a very good principle.                                       

  