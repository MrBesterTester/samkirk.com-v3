---
id: REQ-070
title: Strip chunk references from interview chat output
status: completed
created_at: 2026-02-14T09:50:00Z
user_request: UR-011
claimed_at: 2026-02-14T10:00:00Z
route: B
completed_at: 2026-02-14T10:26:00Z
---

# Strip chunk references from interview chat output

## What
The interview chat assistant responses are leaking internal chunk references like `(chunk_42, chunk_14)` into the user-facing output. These internal identifiers should be stripped before displaying the response to the user.

## Context
- The LLM is including `(chunk_N, chunk_N)` parenthetical references in its generated text, likely because the system prompt includes chunk IDs for citation tracking
- Example output: "...hardware testing, and data visualization. (chunk_42, chunk_14)"
- These references appear at the end of paragraphs in the chat dialog on the Hire Me page
- The system prompt is built in `buildInterviewSystemPrompt()` in `src/lib/interview-chat.ts` with `includeChunkIds: true` passed to `getResumeContext()`
- Two possible fix approaches:
  1. **Post-process**: Strip `(chunk_N, ...)` patterns from the LLM response before returning to the client
  2. **Prompt fix**: Adjust the system prompt to instruct the LLM not to include chunk references in its output
- Both approaches may be needed — prompt instruction to prevent it, plus a regex strip as a safety net

---
*Source: References to chunking are now occurring in the output dialog.*

---

## Triage

**Route: B** - Medium

**Reasoning:** Clear bug fix — strip chunk refs from output. File is named but need to trace the response path to find the right place for the regex strip.

**Planning:** Not required

## Plan

**Planning not required** - Route B: Exploration-guided implementation

*Skipped by work action*

## Exploration

- `interview-chat.ts:547`: `getResumeContext({ format: "detailed", includeChunkIds: true })` — chunk IDs injected into system prompt
- `resume-context.ts:84`: Formats chunks as `[CHUNK 1: Title] (ID: chunk_abc123)`
- `interview-chat.ts:187-239`: System prompt builder — no current instruction against including refs
- `interview-chat.ts:626-631`: LLM call — response captured with leaked refs
- `interview-chat.ts:664-668`: `ChatMessage` creation — refs still in `content`
- API route (`api/tools/interview/route.ts:273-305`) passes refs through to client
- Chunk IDs exist for backend citation tracking (`generateCitationsFromChunks()`), not for LLM output
- Regex pattern: `\s*\(chunk_[\w]+(?:,\s*chunk_[\w]+)*\)\s*`
- Three changes needed in `interview-chat.ts`: (1) add `stripChunkReferences()` utility, (2) add prompt instruction, (3) call strip after LLM response ~line 631

*Generated by Explore agent*

## Implementation Summary

- Added `stripChunkReferences()` utility function (~line 109) — regex strips `(chunk_N, ...)` patterns from text
- Added behavioral guideline #9 "No Internal Identifiers" to system prompt (~line 243) — instructs LLM not to leak chunk refs
- Applied `stripChunkReferences()` to LLM response (~line 648) — `assistantContent = stripChunkReferences(result.text)`
- Added 13 new tests in `interview-chat.test.ts` (11 for strip utility, 1 for prompt instruction, 1 integration test)

*Completed by work action (Route B)*

## Testing

**Tests run:** `npx vitest run src/lib/interview-chat.test.ts`
**Result:** ✓ All 57 tests passing

**New tests added:**
- 11 tests for `stripChunkReferences()` — single/multiple refs, edge cases, non-chunk parens preserved
- 1 test verifying system prompt contains the no-identifiers guideline
- 1 integration test verifying `processMessage` strips refs from LLM output

*Verified by work action*
